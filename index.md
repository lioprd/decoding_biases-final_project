* [Introduction](#introduction)
* [Literature review and approach](#literature-review-and-approach)
* [Methodology](#methodology)
* [Findings and Analysis](#findings and analysis)
* [Project scope](#scope)
* [References](#references)

## Introduction
<p align="justify">The introduction of GPT-3 and other neural network machine learning models reignited a tremendous interest regarding Artificial Intelligence and its potentials in various domains of application. Recently, AI generated art has experienced a wave of popularity, allowing users to create paintings, music, poetry, and eventually raising a lot of questions in the art community and in ethical debates. Our object of study, “AI Gahaku” is an AI artist who creates paintings in numerous styles based on photos submitted by user. Created by the Japanese full-stack developer Sato, the AI is most likely based on an unpaired image-to-image translation model using Generative Adversarial Networks (GANs). These models can produce compelling results even when paired training data is lacking as they only require unlabelled input and output data (Zhu et al., 2020). This technology, if highly impressive and entertaining, has shown some limitations, especially in face-transforming AI platforms, such as AI Gahaku. The most striking and serious limitation might be the racial biases displayed in AI Gahaku and similar apps. Indeed, when presented with pictures from non-white individuals, the AI model tends to be  remove “non-white” features in the generated images. Thus, following a qualitative methodological approach, our study aims to observe the extent to which face-transforming AIs have a bias towards minorities. Such phenomenon resonates greatly with broader social and racial issues, especially processes of underrepresentation and the invisibilization of minority groups. </p>

## Literature review and approach
<p align="justify">As mentioned earlier, we are investigating the possible racial biases present in the AI platform “AI Gahaku”. This application offers to its users the possibility to create a seemingly painted portrait, based on a picture, according to various classical methods of painting. Such offer is contained in the scope of “generative art”. Generative art as a whole refers to “art that in part or in whole has been created by the use of an autonomous system” (Srinivasan and Uchino, 2020), more precisely, the particular structure of “AI Gahaku” falls under the scope of the most common sub-genre of generative art which is computer generated art, more commonly called CG-Art. CG-Art describes an artwork generated by a computer program running by itself, with no interference from a human artist (Boden and Edmonds, 2009), thus mostly by AI. This fairly recent form of art creation has hit a wave of popularity in the late 2020s, with noticeably the commercialization of various apps generating “painted” portraits, such as the object of our study “AI Gahaku” in 2019. In terms of structure, we can safely identify “AI Gahaku” as a Generative Adversarial Network (GAN) (Ian J. Goodfellow et al., 2014). This type of machine learning is based on two datasets learning from each other unsupervised, creating a new product, it can be an image or a music for example, in the same style as the training set which could explain the popularity of GANs in platforms resembling “AI Gahaku”. </p>
  
<p align="justify">	The phenomenon of generative art raised a lot of questions regarding the sheer essence of art; can a computer truly create a piece of art ? This debate, otherwise fascinating, is not the matter of our analysis but more of a starting point. Indeed, some authors, such as Aaron Hertzmann, argue that the process of creating “art” is primarily social, and thus can not be achieved by a computer program such an AI (Hertzmann, 2018). But if generated art is not driven by social consideration, we argue that it may reflect social concerns and issues, and in the case of AI Gahaku racial biases and white washing. The main general argument for such biases could be the pre-existing human prejudices and preconceptions embedded in the dataset (Young, 2019). In scientific reasoning, we rely heavily on Ramya Srinivasan and Kanji Uchino’s theory regarding the reasons behind such racial biases in generative portrait AI. The authors go into details about two types of bias in the algorithm that can result in addition of the dataset bias in such issues, which are the transportability bias and the selection bias. The latter refers to the bias induced in the algorithm by a preferential selection of units in the data set. The authors go on to give the example of AI generating Renaissance style portraits. The AI will operate a selection of input images composed of Renaissance paintings, overwhelmingly representing white persons and thus, if given a picture of a person of color, will tend to whitewash them in the generated portrait. Transportability, on the other hand, defines “the conditions under which causal effects learned in experimental studies can be transferred into a new population in which only observational studies can be conducted.” This process can result in racial biases if the causal effects are not transferable across populations, which is observed by the authors when a specific AI, “GoArt”, used to convert photos in various styles, tints in blue or red the face of The Black Matriarch, a painting by Clementine Hunter, when asked to convert it in Expressionist or Byzantine style. </p>
  
<p align="justify"> The racial biases observed by the authors in CG-art and by ourselves in the case of AI-Gahaku have serious repercussions. As the artist Nina Baldwin beautifully put “Art is power. It can influence perception, opinion, and values.”, a racially biased CG-Art could thus create and communicate unethical values and uphold phenomenons of invisibilization of marginalized groups.</p>

## Methodology

Evaluating the biases of an AI art generator - AI Gahaku which generates painting in different styles based on a picture.

Our aim is to identify whether the AI underperforms when creating images based on pictures of non-Caucasian people. The first stage of our research consisted in gathering a dataset of faces from different ethnicities. To do so, we relied on the work of Kärkkäinen and Joo (2021) who constructed a dataset aiming at solving the biases that other public face image datasets had toward Caucasian faces. They argue that most datasets tend to significantly underrepresent non-Caucasian faces and created an inclusive dataset to mitigate the race bias problem (Kärkkäinen and Joo, 2021, p. 1548). They defined the following seven race groups: Black, East Asian, Indian, Latino, Middle Eastern, Southeast Asian and White. We created 3 groups out of these seven categories, two treatment groups: Black and East Asian and a control group: White. As an overwhelming majority of Northern Renaissance Paintings feature white people, we assume that their representation will be the most accurate. We picked two groups that are typically underrepresented in this style to help us to assess whether a bias does indeed exist, and if yes whether it is similar for both groups or not. 
We selected a total of 45 pictures (15 per groups) and generated portraits using the AI-Gahaku mobile application. Furthermore, to evaluate whether the biases where only limited to one specific type of painting, we di


## Findings & Analysis

  

<p align="justify"> Before delving into the mismatches in the quality of the generations that could have been generated by a bias in the AI model, we need to address some aspects affecting all generations. It can notably be noticed that the shapes of the faces tend to be distorted or to present some artefacts (e.g.: #50, #2061, #4228). This is most likely due to formatting of the pictures in the fairface dataset, which crops pictures to only leave faces on the picture. As AI Gahaku generates portraits, it expands some cropped image to try and generate a full face, sometimes deforming faces. As our research focuses on biases in the faces of individuals, it will not be problematic for our analysis. Furthermore, the model produces subpar results for people with an open mouth, which occurred for some portraits, but it does not indicate a bias of the model. Another difficulty introduced by the fairface dataset is that the pictures are relatively small as the minimum size of a detected face can start at 50 by 50 pixels However, as mentioned by Kärkkäinen & Joo (2021, p. 1551), attributes are still recognizable so generations are still accurate enough to assess the presence of biases or not.
  
<p align="justify"> We will first assess the presence of biases on the Early Renaissance generations and followingly examine whether Contemporary Realism generations were less biased. As predicted by our initial hypothesis, the generated paintings show a certain degree of biases. The first group examined; Black people shows the most obvious bias which is linked to the skin tones of the generations. As can be seen on the figure XXX, all generations feature rather similar skin colours, which were used in Early Italian Renaissance by painters such as Antonello de Messina to depict white people. The gap is less striking on the generations that were made based on the East Asian group as their skin tones are less contrasted with the generated tone. However, after looking more in-depth at our generations, we can see that different undertones tend to be erased and the three group are uniformly depicted with white skin tones.

<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/Figure_1.png"> <br>
  
<b>Original pictures and generations *Early Renaissance*, references of the images (link to right and top to bottom): #412, #1019, #4433, #1719</b>

<p align="justify">Besides the complete change of complexion, we observed that the model reproduces the face shape and most facial characteristics of the people rather accurately. To observe whether significant changes occurred, we juxtaposed original pictures with the AI generated images and observed which areas were misrepresented. We noted that eye shapes tend to not be represented accurately, especially for some members of the East Asian group. For instance, the epicanthal folds on the portraits #3718, #3623, #3974 and #4251 are replaced by rounder, greener eyes on the generated painting.  
  
<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/Figure_2.png">

<b>Portrait #3718 & #3623 and juxtaposition of the real picture (black frame) on the generated portrait)</b> 

<p align="justify">Such modifications seem to be less prevalent when looking at our control group (white people) even though some individuals present artefacts around their eyes. A similar issue occurs for some portraits of the first group. 

<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_3.png">

<b> Portrait #119 & #5933 and juxtaposition of the real picture (black frame) on the generated portrait)</b>

<p align="justify">The issues abovementioned contribute to show that the portraits of white people tend to be more accurately represented than the other groups or at least that the respective generated pictures are closer to the original models. As the canonical aesthetic of Early Italian Renaissance is almost entirely based on the depiction of white people, this is unsurprising. 
  
<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_4.png">
  
<b>Original pictures and generations, references of the images (link to right and top to bottom): #654, #623, #37, #3870</b>

<p align="justify">The paintings used by AI Gahaku to train the model are most likely based on Antonello da Messina’s work, as one of his paintings is used illustrate the style of images that will be produced when selecting the style. After checking his work through the art database Wikiart (2022) , we did not find a single painting representing a non-white person. This could potentially explain the shortcomings of the models to generate pictures representing people from different ethnicities. 
As mentioned in our methodological part, to see whether those issues would be mitigated by applying an art style that potentially used more inclusive dataset for training, we subsequently generated portraits using the Contemporary Realism style. Amongst the options offered by AI Gahaku, we hypothesized that it could potentially produce less biased results than Early Renaissance. 
The results are, however, practically identical. This is notably visible when looking at the skin complexions of the second round of generations. The complexions are similar to the sample image provided for Contemporary Realism but disregards the skin colour of the pictures selected to produce a white complexion. 

 <img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_5.png">

<b>Original pictures and generations *Contemporary Realism*, references of the images (link to right and top to bottom): #120, #1370, #4605, #6765</b>
  
<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_6.png">

The previous findings on eye shapes remain similar with the images generated with the Contemporary Realism. Frome these results, we can infer that the training data used for Contemporary Realism also lacked inclusivity. However, to better understand the causes of the biases produced by AI Gahaku, it is necessary to describe more precisely how the generations were created. 
  
AI Gahaku creator’s does not provide any explanation pertaining to the type of model used to create the paintings. The most likely way to create such generations would be through unpaired image-to-image translation using generative adversarial networks (GAN). One of the most widespread technologies to generate art are paired adversarial network. GANs are based on the interaction between generative neural network and a discriminator one. The generative network creates images attempting to recreate the demanded output convincingly and those are passed through the discriminator network which attempt to identify whether they are accurate or not. Both networks learn and improve together and use pairs of images (inputs and outputs) to learn the expected results. 
  
In the case of AI Gahaku, such pairs do not exist as the paintings that are replicated have no direct picture equivalents. Zhu & al. (2017) explain that another method allows to capture the characteristics of an input dataset and translate it into an output dataset without having paired, labelled images. They use unpaired training data: consisting of a source set<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/equation.png"> and a target set <img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/equation2.png">with no information provided as to which x_i matches which y_j (Zhu & al. 2017). Using unpaired data of two datasets with similar characteristics is sufficient to train models to convincingly transfer the style of the base dataset to the model.

<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/img_translation.jpg">
<b>Paired and unpaired training data, <i> retrieved from Zhu & Al. 2017, p. 2</i> </b>
  
  
AI Gahaku’s model was highly likely also trained with unpaired data, using paintings as the input X and portraits as output Y. By looking at our findings, we can infer that not only the portraits used to train the model excluded non-white people but it is likely that output images were also biased. 

  </p>




## Limitations 

{\lbrace{x_{i}}\rbrace}_{i=1}^N (x_i \in X)
{\lbrace{y_{j}}\rbrace}_{j=1}^M (y_j \in Y)

## Annex 

## References
