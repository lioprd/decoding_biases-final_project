# Racial Biases in AI Gahaku and Image-to-image GAN Art 
_How can AI Gahaku and Image-to-Image GAN art reproduce racial biases?_  

* [Introduction](#introduction)
* [Literature review and approach](#literature-review-and-approach)
* [Methodology](#methodology)
* [Findings](#findings)
* [Discussion](#discussion)
* [References](#references)
* [Annex](#annex)

## Introduction
<p align="justify">The introduction of GPT-3 and other neural network machine learning models reignited a tremendous interest regarding Artificial Intelligence and its potentials in various domains of application. Recently, AI generated art has experienced a wave of popularity, allowing users to create paintings, music, poetry, and eventually raising a lot of questions in the art community and in ethical debates. Our object of study, “AI Gahaku” is an AI artist who creates paintings in numerous styles based on photos submitted by user. Created by the Japanese full-stack developer Sato, the AI is most likely based on an unpaired image-to-image translation model using Generative Adversarial Networks (GANs). These models can produce compelling results even when paired training data is lacking as they only require unlabelled input and output data (Zhu et al., 2020). This technology, if highly impressive and entertaining, has shown some limitations, especially in face-transforming AI platforms, such as AI Gahaku. The most striking and serious limitation might be the racial biases displayed in AI Gahaku and similar apps. Indeed, when presented with pictures from non-white individuals, the AI model tends to be  remove “non-white” features in the generated images. Thus, following a qualitative methodological approach, our study aims to observe the extent to which face-transforming AIs have a bias towards minorities. Such phenomenon resonates greatly with broader social and racial issues, especially processes of underrepresentation and the invisibilization of minority groups. </p>

## Literature review and approach
<p align="justify">As mentioned earlier, we are investigating the possible racial biases present in the AI platform “AI Gahaku”. This application offers to its users the possibility to create a seemingly painted portrait, based on a picture, according to various classical methods of painting. Such offer is contained in the scope of “generative art”. Generative art as a whole refers to “art that in part or in whole has been created by the use of an autonomous system” (Srinivasan and Uchino, 2020), more precisely, the particular structure of “AI Gahaku” falls under the scope of the most common sub-genre of generative art which is computer generated art, more commonly called CG-Art. CG-Art describes an artwork generated by a computer program running by itself, with no interference from a human artist (Boden and Edmonds, 2009), thus mostly by AI. This fairly recent form of art creation has hit a wave of popularity in the late 2020s, with noticeably the commercialization of various apps generating “painted” portraits, such as the object of our study “AI Gahaku” in 2019. In terms of structure, we can safely identify “AI Gahaku” as a Generative Adversarial Network (GAN) (Ian J. Goodfellow et al., 2014). This type of machine learning is based on two datasets learning from each other unsupervised, creating a new product, it can be an image or a music for example, in the same style as the training set which could explain the popularity of GANs in platforms resembling “AI Gahaku”. </p>
  
<p align="justify">	The phenomenon of generative art raised a lot of questions regarding the sheer essence of art; can a computer truly create a piece of art ? This debate, otherwise fascinating, is not the matter of our analysis but more of a starting point. Indeed, some authors, such as Aaron Hertzmann, argue that the process of creating “art” is primarily social, and thus can not be achieved by a computer program such an AI (Hertzmann, 2018). But if generated art is not driven by social consideration, we argue that it may reflect social concerns and issues, and in the case of AI Gahaku racial biases and white washing. The main general argument for such biases could be the pre-existing human prejudices and preconceptions embedded in the dataset (Young, 2019). In scientific reasoning, we rely heavily on Ramya Srinivasan and Kanji Uchino’s theory regarding the reasons behind such racial biases in generative portrait AI. The authors go into details about two types of bias in the algorithm that can result in addition of the dataset bias in such issues, which are the transportability bias and the selection bias. The latter refers to the bias induced in the algorithm by a preferential selection of units in the data set. The authors go on to give the example of AI generating Renaissance style portraits. The AI will operate a selection of input images composed of Renaissance paintings, overwhelmingly representing white persons and thus, if given a picture of a person of color, will tend to whitewash them in the generated portrait. Transportability, on the other hand, defines “the conditions under which causal effects learned in experimental studies can be transferred into a new population in which only observational studies can be conducted.” This process can result in racial biases if the causal effects are not transferable across populations, which is observed by the authors when a specific AI, “GoArt”, used to convert photos in various styles, tints in blue or red the face of The Black Matriarch, a painting by Clementine Hunter, when asked to convert it in Expressionist or Byzantine style. </p>
  
<p align="justify"> The racial biases observed by the authors in CG-art and by ourselves in the case of AI-Gahaku have serious repercussions. As the artist Nina Baldwin beautifully put “Art is power. It can influence perception, opinion, and values.”, a racially biased CG-Art could thus create and communicate unethical values and uphold phenomenons of invisibilization of marginalized groups.</p>

## Methodology

<b>Approach</b>
<p align="justify">Given that AI Gahaku’s code, similarly to that of many other CG-art, is not open-source, we are attempting to understand how the algorithm functions by studying its inputs and studying with the results/outputs generated by the algorithm.</p>

<b>Methodology </b>
<p align="justify">Our aim was to evaluate he accuracy of face-transforming AI on a diversity of faces by testing AI Gahaku’s accuracy on the faces of people of different ethnicities and identify whether it underperforms when creating images based on pictures of non-Caucasian people. </p>

<p align="justify"> <b> 1. </b> The first stage of our research consisted in gathering a dataset of faces from different ethnicities. To do so, we relied on the work of Kärkkäinen and Joo (2021) who constructed FairFace, a face attribute dataset balanced for race, gender, and age in the aim to resolve the biases that other public face image datasets such as previous datasets had toward Caucasian faces. They argue that most datasets tend to significantly underrepresent non-Caucasian faces and created an inclusive dataset to mitigate the race bias problem (Kärkkäinen and Joo, 2021, p. 1548). They defined the following seven race groups: Black, East Asian, Indian, Latino, Middle Eastern, Southeast Asian and White. We created 3 groups out of these seven categories, two treatment groups: Black and East Asian and a control group: White. We selected a total of 45 pictures (15 per ethnic group).

<p align="justify"> We assumed that since an overwhelming majority of the Northern Renaissance paintings feature white people, their representation will be the most accurate. For the control groups, we picked two groups that are typically underrepresented in this style to help us to assess whether a bias does indeed exist, and if yes whether it is similar for both groups or not.<br>

<p align="justify"> <b> 2. </b>  Second, we conducted research on various painting movements and eras, featured as filters by AI Gahaku in order to identify the two most relevant styles for comparison and any potential racial biases stemming from the artistic movement, rather than the algorithm itself. We settled on Early Renaissance (specifically the ME1 filter inspired by a painting by Antonello de Messine) and Contemporary Realism (specifically the WH2 filter) because the former is known for representing primarily European faces, while the latter style features more diversity in its portraits.
<div align="center"><img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decoding_biases-final_project/main/assets/wh2me1.png"></div> 
<div align="center"><b>Preview image of the two styles on the AI Gahaku app </b></div>
<p align="justify"> We chose to compare the portraits generated through two different filters with different reputations regarding diversity, in order to better understand if any potential biases could be coming from the algorithm or rather from the filter’s dataset lacking diversity. If the difference in accuracy between the Early Renaissance paintings compared to the Contemporary Realism portraits were shocking, it would suggest the Early Renaissance dataset might be the root cause.

<p align="justify"><b> 3. </b> Finally, we converted the 45 faces into Renaissance paintings and into Contemporary Realism style paintings, comparing their accuracy amongst themselves and in comparison to the original faces. We also juxtaposed some of the original faces onto the AI-generated portraits to evaluate whether the face structure was deformed. </p>

## Findings

<p align="justify"> Before delving into the mismatches in the quality of the generations that could have been generated by a bias in the AI model, we need to address some aspects affecting all generations. It can notably be noticed that the shapes of the faces tend to be distorted or to present some artefacts (e.g.: #50, #2061, #4228). This is most likely due to formatting of the pictures in the fairface dataset, which crops pictures to only leave faces on the picture. As AI Gahaku generates portraits, it expands some cropped image to try and generate a full face, sometimes deforming faces. As our research focuses on biases in the faces of individuals, it will not be problematic for our analysis. Furthermore, the model produces subpar results for people with an open mouth, which occurred for some portraits, but it does not indicate a bias of the model. Another difficulty introduced by the fairface dataset is that the pictures are relatively small as the minimum size of a detected face can start at 50 by 50 pixels However, as mentioned by Kärkkäinen & Joo (2021, p. 1551), attributes are still recognizable so generations are still accurate enough to assess the presence of biases or not.
  
<p align="justify"> We will first assess the presence of biases on the Early Renaissance generations and followingly examine whether Contemporary Realism generations were less biased. As predicted by our initial hypothesis, the generated paintings show a certain degree of biases. The first group examined; Black people shows the most obvious bias which is linked to the skin tones of the generations. As can be seen on the figure XXX, all generations feature rather similar skin colours, which were used in Early Italian Renaissance by painters such as Antonello de Messina to depict white people. The gap is less striking on the generations that were made based on the East Asian group as their skin tones are less contrasted with the generated tone. However, after looking more in-depth at our generations, we can see that different undertones tend to be erased and the three group are uniformly depicted with white skin tones.

<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/Figure_1.png">
  
<b>Original pictures and generations *Early Renaissance*, references of the images (link to right and top to bottom): #412, #1019, #4433, #1719</b>

<p align="justify">Besides the complete change of complexion, we observed that the model reproduces the face shape and most facial characteristics of the people rather accurately. To observe whether significant changes occurred, we juxtaposed original pictures with the AI generated images and observed which areas were misrepresented. We noted that eye shapes tend to not be represented accurately, especially for some members of the East Asian group. For instance, the epicanthal folds on the portraits #3718, #3623, #3974 and #4251 are replaced by rounder, greener eyes on the generated painting.  
  
<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/Figure_2.png">

<b>Portrait #3718 & #3623 and juxtaposition of the real picture (black frame) on the generated portrait)</b> 

<p align="justify">Such modifications seem to be less prevalent when looking at our control group (white people) even though some individuals present artefacts around their eyes. A similar issue occurs for some portraits of the first group. 

<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_3.png">

<b> Portrait #119 & #5933 and juxtaposition of the real picture (black frame) on the generated portrait)</b>

<p align="justify">The issues abovementioned contribute to show that the portraits of white people tend to be more accurately represented than the other groups or at least that the respective generated pictures are closer to the original models. As the canonical aesthetic of Early Italian Renaissance is almost entirely based on the depiction of white people, this is unsurprising. 
  
<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_4.png">
  
<b>Original pictures and generations, references of the images (link to right and top to bottom): #654, #623, #37, #3870</b>

<p align="justify">The paintings used by AI Gahaku to train the model are most likely based on Antonello da Messina’s work, as one of his paintings is used illustrate the style of images that will be produced when selecting the style. After checking his work through the art database Wikiart (2022) , we did not find a single painting representing a non-white person. This could potentially explain the shortcomings of the models to generate pictures representing people from different ethnicities. 
As mentioned in our methodological part, to see whether those issues would be mitigated by applying an art style that potentially used more inclusive dataset for training, we subsequently generated portraits using the Contemporary Realism style. Amongst the options offered by AI Gahaku, we hypothesized that it could potentially produce less biased results than Early Renaissance. 
The results are, however, practically identical. This is notably visible when looking at the skin complexions of the second round of generations. The complexions are similar to the sample image provided for Contemporary Realism but disregards the skin colour of the pictures selected to produce a white complexion. 

 <img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_5.png">

<b>Original pictures and generations *Contemporary Realism*, references of the images (link to right and top to bottom): #120, #1370, #4605, #6765</b>
  
<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/figure_6.png"><b> Portrait #3718 & #3623 and juxtaposition of the real picture (black frame) on the generated portrait – <i> Contemporary Realism </i> </b>

The previous findings on eye shapes remain similar with the images generated with the Contemporary Realism. Frome these results, we can infer that the training data used for Contemporary Realism also lacked inclusivity. However, to better understand the causes of the biases produced by AI Gahaku, it is necessary to describe more precisely how the generations were created. 
  
AI Gahaku creator’s does not provide any explanation pertaining to the type of model used to create the paintings. The most likely way to create such generations would be through unpaired image-to-image translation using generative adversarial networks (GAN). One of the most widespread technologies to generate art are paired adversarial network. GANs are based on the interaction between generative neural network and a discriminator one. The generative network creates images attempting to recreate the demanded output convincingly and those are passed through the discriminator network which attempt to identify whether they are accurate or not. Both networks learn and improve together and use pairs of images (inputs and outputs) to learn the expected results. 
  
In the case of AI Gahaku, such pairs do not exist as the paintings that are replicated have no direct picture equivalents. Zhu & al. (2017) explain that another method allows to capture the characteristics of an input dataset and translate it into an output dataset without having paired, labelled images. They use unpaired training data: consisting of a source set<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/equation.png"> and a target set <img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/equation2.png">with no information provided as to which x_i matches which y_j (Zhu & al. 2017). Using unpaired data of two datasets with similar characteristics is sufficient to train models to convincingly transfer the style of the base dataset to the model.

<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/assets/img_translation.jpg">
<b>Paired and unpaired training data, retrieved from Zhu & Al. 2017, p. 2</b>
  
<p>AI Gahaku’s model was highly likely also trained with unpaired data, using paintings as the input X and portraits as output Y. By looking at our findings, we can infer that not only the portraits used to train the model excluded non-white people but it is likely that output images were also biased. </p>

## Discussion 

<img style="display: block;-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decoding_biases-final_project/main/assets/image.png">
## Limitations

Some text



## References

- Srinivasan, Ramya. Uchino, Kanji. “Biases in Generative Art— A Causal Look from the Lens of Art History”, FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. March 2021.  Pages 41–51: https://doi.org/10.1145/3442188.3445869

- Edmonds, Ernest A.. Boden, Margaret A.. “What is generative art”, Digital Creativity. March 2009: DOI: 10.1080/14626260902867915
Hertzmann, Aaron.. "Can Computers Create Art?" Arts 7, no. 2: 18. February 2018 https://doi.org/10.3390/arts7020018

- Goodfellow, Ian J. Pouget-Abadie, Jean. Mirza, Mehdi. Xu, Bing. Warde-Farley, David. Ozair, Sherjil. Courville, Aaron. Bengio, Yoshua. “Generative Adversarial Networks”, Arxiv. June 2014: https://doi.org/10.48550/arXiv.1406.2661

- Karkkainen, K., & Joo, J. (2021). FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 1548-1558).

- Young, David. Tabula Rasa: Rethinking the Intelligence of Machine Minds. 2019 https://medium.com/@dkyy/tabula-rasa-b5f846e60859 

- Rea, Naomi. “How ImageNet Roulette, an Art Project That Went Viral by Exposing Facial Recognition’s Biases, Is Changing People’s Minds About AI.”, Artnet News. September 2019: https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305

- Hardesty, Larry. “Study finds gender and skin-type bias in commercial artificial-intelligence systems”, MIT News. February 2018: https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212

- Jun-Yan Zhu*, Taesung Park*, Phillip Isola, and Alexei A. Efros. "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks", in IEEE International Conference on Computer Vision (ICCV), 2017.


## Annex 
<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decodingbiases/main/1%20-%20Black%20-%20All%20generations.png" >
<b>Group 1 - Black - Generations in both styles and original pictures</b> <br>

<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decoding_biases-final_project/main/2%20-%20East%20Asian%20-%20All%20generations.png" >
<b>Group 2 - East Asian - Generations in both styles and original picture</b> <br>

<img style="display: block;-webkit-user-select: none;margin: auto;cursor: zoom-in;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://raw.githubusercontent.com/lioprd/decoding_biases-final_project/main/3%20-%20White%20-%20All%20generations.png">
<b>Group 3 - White - Generations in both styles and original picture</b> <br>

